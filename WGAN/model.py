# -*- coding: utf-8 -*-
"""wGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U8an1o2UYZPNhxRyGsb6BQTg0e8MW1W2
"""

import os, ssl
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

if (not os.environ.get('PYTHONHTTPSVERIFY', '') and
    getattr(ssl, '_create_unverified_context', None)): 
    ssl._create_default_https_context = ssl._create_unverified_context
    
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("./mnist/data/", one_hot=True)

TrainImage = mnist.train.images
print (np.shape(TrainImage))

"""
tf.layers.conv2d(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format='channels_last',
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None
)

tf.layers.conv2d_transpose(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format='channels_last',
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None
)
"""

# Generator Network
# 노이즈 디멘션은 100
# 7*7*128 차원으로 확장하고 reshape
# (7, 7, 128) -> (14, 14, 64) -> (28, 28, 32) -> (28, 28, 1)
def Build_Generator (inputs):
    
    # 이하 Generator 관여하는 모든 변수는 variable_scope로 묶어요.
    with tf.variable_scope("Generator_Varaible"):
        
        output = tf.layers.dense(inputs, 7*7*128)
        output = tf.reshape(output, [-1, 7, 7, 128])
        output = tf.layers.batch_normalization(output, training = IsTraining)
        output = tf.nn.relu (output)
        
        output = tf.layers.conv2d_transpose(output, filters = 64, 
                                            kernel_size = KernelSize1, 
                                            strides = (2, 2), 
                                            padding = "SAME", 
                                            use_bias = False)
        output = tf.layers.batch_normalization(output, training = IsTraining)
        output = tf.nn.relu (output)
        
        output = tf.layers.conv2d_transpose(output, filters = 32, 
                                            kernel_size = KernelSize1, 
                                            strides = (2, 2), 
                                            padding = "SAME", 
                                            use_bias = False)
        output = tf.layers.batch_normalization(output, training = IsTraining)
        output = tf.nn.relu (output)
        
        output = tf.layers.conv2d_transpose(output, filters = 1, 
                                            kernel_size = KernelSize1, 
                                            strides = (1, 1), 
                                            padding = "SAME", 
                                            use_bias = False)
        output = tf.tanh (output)
        
    return output



# (28, 28, 1) -> (14, 14, 32) -> (7, 7, 64) -> (4, 4, 128)
# flatten to dimension 1
# Discriminator Function
def Build_Discriminator (inputs, reuse = None):
    
    # 이하 Dirscriminator에 관여하는 모든 변수는 variable_scope로 묶어요.
    with tf.variable_scope("Discriminator_Variable") as scope:
        
        if reuse:
            scope.reuse_variables()
            
        output = tf.reshape (inputs, [-1, 28, 28, 1])
        output = tf.layers.conv2d(output, filters = 32, 
                                  kernel_size = KernelSize1, 
                                  strides = (2, 2), 
                                  padding = "SAME", 
                                  use_bias = True,
                                  kernel_initializer = tf.glorot_uniform_initializer())
        output = tf.nn.leaky_relu (output)
        
        output = tf.layers.conv2d(output, filters = 64, 
                                  kernel_size = KernelSize1, 
                                  strides = (2, 2), 
                                  padding = "SAME", 
                                  use_bias = False,
                                  kernel_initializer = tf.glorot_uniform_initializer())
        output = tf.layers.batch_normalization(output, training = IsTraining)
        output = tf.nn.leaky_relu (output)
        
        output = tf.layers.conv2d(output, filters = 128, 
                                  kernel_size = KernelSize1, 
                                  strides = (2, 2), 
                                  padding = "SAME", 
                                  use_bias = False,
                                  kernel_initializer = tf.glorot_uniform_initializer())
        output = tf.layers.batch_normalization(output, training = IsTraining)
        output = tf.nn.leaky_relu (output)
        
        output = tf.layers.flatten(output)
        output = tf.layers.dense(output, 1, activation = None)
        
    return output



# Noise Function
def Build_GetNoise (batch_size, noise_size):
    
    outputs = np.random.uniform(-1.0, 1.0, size = [batch_size, noise_size])
    
    return outputs

# Noise Function
def Build_GetNoise_forNormal (batch_size, noise_size):
    
    outputs = np.random.normal(-1.0, 1.0, size = [batch_size, noise_size])
    
    return outputs

# 배치사이즈를 정의
# 전체 데이터를 활용할 에포크 크기를 정의
# 노이즈 디멘션의 길이를 정의
BatchSize = 128
TotalEpoch = 100
NoiseDimension = 100
KernelSize1 = (3, 3)
KernelSize2 = (5, 5)

# Variable GeneratorVal/dense/kernel already exists, disallowed. 해결 방법
# tf.reset_default_graph()를 그래프 맨 앞에 적어두기
tf.reset_default_graph()
 
DiscriminInput = tf.placeholder(tf.float32, [None, 784])
GeneratorInput = tf.placeholder(tf.float32, [None, NoiseDimension])
IsTraining = tf.placeholder(tf.bool)

DiscGlobalStep = tf.Variable(0, trainable = False, name = "DiscGlobal")
GeneGlobalStep = tf.Variable(0, trainable = False, name = "GeneGlobal")

FakeImage = Build_Generator(GeneratorInput)

# Discriminaotor에 실제 이미지를 먼저 넣는다.
# 그리고 사용한 모수를 재활용하여 FakeImage를 입력한다.
DiscReal = Build_Discriminator(DiscriminInput)
DiscGene = Build_Discriminator(FakeImage, True)

DiscriminLoss = tf.reduce_mean(DiscReal) - tf.reduce_mean(DiscGene)
GeneratorLoss = tf.reduce_mean(DiscGene)


# tf.get_collection과 tf.GraphKeys.TRAINABLE_VARIABLES with "scope 이름" 을 이용하여
# 서로 다른, 독립의 변수 묶음을 정의. GAN 구조에서는 Discriminator 변수와 Generator 변수를 겹치지 않는 변수로 간주
# Discriminator의 변수와 Generator의 변수는 따로 학습시킨다.
# tf.control_dependecies는 묶음 연산과 실행 순서를 정의하는 메서드이다.
# UpdataOps를 먼저 실행하고 TrainDisc, TrainGene을 실행한다.
DiscVars = tf.get_collection (tf.GraphKeys.TRAINABLE_VARIABLES, scope = "Discriminator_Variable")
GeneVars = tf.get_collection (tf.GraphKeys.TRAINABLE_VARIABLES, scope = "Generator_Varaible")
UpdateOps = tf.get_collection (tf.GraphKeys.UPDATE_OPS)

# Weight Clipping
ClippingDiscVar = [Vars.assign(tf.clip_by_value(Vars, -0.2, 0.2)) for Vars in DiscVars]

with tf.control_dependencies(UpdateOps):
    TrainDisc = tf.train.AdamOptimizer(learning_rate = 0.001, beta1 = 0.5).\
                    minimize(DiscriminLoss, var_list=DiscVars, global_step = DiscGlobalStep)
    TrainGene = tf.train.AdamOptimizer(learning_rate = 0.003, beta1 = 0.5).\
                    minimize(GeneratorLoss, var_list=GeneVars, global_step = GeneGlobalStep)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    TotalBatch = int(mnist.train.num_examples / BatchSize)
    
    for epoch in range(TotalEpoch):
        
        DiscLossValue = 0
        GeneLossValue = 0
        
        for i in range(TotalBatch):
            
            batch_xs, batch_ys = mnist.train.next_batch(BatchSize)
            Noise = Build_GetNoise(BatchSize, NoiseDimension)
            
            feed_dicting = {DiscriminInput : batch_xs, GeneratorInput : Noise, IsTraining : True}

            # Discriminator는 원본 이미지와 노이즈가 생성한 이미지를 모두 받은 후에 학습하여서 feed_dict가 X, Z 둘 다 필요
            sess.run([TrainDisc, ClippingDiscVar], feed_dict = feed_dicting)
            DiscLossValue = sess.run(DiscriminLoss, feed_dict = feed_dicting)
            
            # Generator는 원본 이미지가 관여하지 않아요. 오직 노이즈만 받으면 됩니다.
            sess.run(TrainGene, feed_dict = feed_dicting)
            GeneLossValue = sess.run(GeneratorLoss, feed_dict = feed_dicting)
         

        print('Epoch:', '%02d   ' %(epoch+1), 
              'D loss: {:.4}   '.format(DiscLossValue), 
              'G loss: {:.4}   '.format(GeneLossValue))

        
        # 랜덤 함수로 노이즈 샘플을 추출하여 Generator가 잘 학습하였는지 확인
        # Latent Space 알고리즘
        """
        시작 리스트, 끝 리스트 생성 (Build_GetNoise_forNormal)
        """
        if epoch % 1 == 0:
            
            Test_Noise = Build_GetNoise(10, NoiseDimension)
            Samples = sess.run(FakeImage, feed_dict = {GeneratorInput : Test_Noise, IsTraining : False})

            
            fig, ax = plt.subplots(1, 10, figsize = (20, 10))

            for i in range(10):
                
                ax[i].set_axis_off()
                ax[i].imshow(np.reshape(Samples[i], (28, 28)))

            plt.show ()
            plt.close(fig)