{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "MuzQiOZHNikW",
    "outputId": "e82d83ec-112e-4aa5-a51e-1a582da42259"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "tf.keras.layers.Conv2D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    data_format=None,\n",
    "    dilation_rate=(1, 1),\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "tf.keras.layers.Conv2DTranspose(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    output_padding=None,\n",
    "    data_format=None,\n",
    "    dilation_rate=(1, 1),\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "tf.reshape(\n",
    "    tensor,\n",
    "    shape,\n",
    "    name=None\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "T_8-ZRAkOFtU",
    "outputId": "95e35334-2297-4a5a-babe-049f5e01101b"
   },
   "outputs": [],
   "source": [
    "tf.__version__\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27AGC8rjOI5P"
   },
   "outputs": [],
   "source": [
    "class LSGAN_model ():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # MNIST image size, channels -> shape && noise_dimenstion size\n",
    "        self.image_rows = 28\n",
    "        self.image_cols = 28\n",
    "        self.image_channels = 1\n",
    "        self.image_shape = (self.image_rows, self.image_cols, self.image_channels)\n",
    "        self.noise_dimension = 100\n",
    "        \n",
    "        # Convolution optional turpel for LSGAN architecture\n",
    "        self.filter1, self.filter2, self.filter3, self.filter4 = 128, 64, 32, 1\n",
    "        self.two_strides = (2, 2)\n",
    "        self.one_strides = (1, 1)\n",
    "        self.kernel_size = (3, 3)\n",
    "        \n",
    "        self.noise_dimenstion = 100\n",
    "        self.learning_rate = 2e-4\n",
    "        \n",
    "        \n",
    "    def build_Generator(self, inputs):\n",
    "        \n",
    "        \"\"\"\n",
    "        # Convolution optional turpel for LSGAN architecture\n",
    "        self.filter1, self.filter2, self.filter3, self.filter4 = 128, 64, 32, 1\n",
    "        self.two_strides = (2, 2)\n",
    "        self.one_strides = (1, 1)\n",
    "        self.kernel_size = (3, 3)\n",
    "        \"\"\"\n",
    "        dense_size = 7*7*128\n",
    "        dense_size_turple = (-1, 7, 7, 128)\n",
    "        \n",
    "        # batch_size에 대해서는 입력하지 않아도 자동으로 none\n",
    "        generator = Dense(dense_size, activation = None) (inputs)\n",
    "        generator = tf.reshape(generator, dense_size_turple)\n",
    "        # generator = BatchNormalization() (generator)\n",
    "        generator = relu(generator)\n",
    "        \n",
    "        ## 2\n",
    "        generator = Conv2DTranspose(\n",
    "                    filters = 128,\n",
    "                    kernel_size = self.kernel_size,\n",
    "                    strides = self.two_strides, \n",
    "                    padding = \"same\") (generator)\n",
    "        # generator = BatchNormalization() (generator)\n",
    "        generator = relu(generator)\n",
    "        \n",
    "        generator = Conv2DTranspose(\n",
    "                    filters = 32,\n",
    "                    kernel_size = self.kernel_size,\n",
    "                    strides = self.two_strides, \n",
    "                    padding = \"same\") (generator)\n",
    "        # generator = BatchNormalization() (generator)\n",
    "        generator = relu(generator)\n",
    "        \n",
    "        generator = Conv2DTranspose(\n",
    "                    filters = 1,\n",
    "                    kernel_size = self.kernel_size,\n",
    "                    strides = self.one_strides, \n",
    "                    padding = \"same\") (generator)\n",
    "        generator = tanh(generator)\n",
    "        \n",
    "        outputs = generator\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def build_Discriminator(self, inputs):\n",
    "        \n",
    "        \"\"\"\n",
    "        # Convolution optional turpel for LSGAN architecture\n",
    "        self.filter1, self.filter2, self.filter3, self.filter4 = 128, 64, 32, 1\n",
    "        self.two_strides = (2, 2)\n",
    "        self.one_strides = (1, 1)\n",
    "        self.kernel_size = (3, 3)\n",
    "        \"\"\"\n",
    "        discriminator = Conv2D(\n",
    "                        filters = self.filter3,\n",
    "                        kernel_size = self.kernel_size,\n",
    "                        strides = self.two_strides,\n",
    "                        padding = \"same\") (inputs)\n",
    "        discriminator = tf.nn.leaky_relu(discriminator)\n",
    "        \n",
    "        discriminator = Conv2D(\n",
    "                        filters = self.filter2,\n",
    "                        kernel_size = self.kernel_size,\n",
    "                        strides = self.two_strides,\n",
    "                        padding = \"same\") (discriminator)\n",
    "        # discriminator = BatchNormalization() (discriminator)\n",
    "        discriminator = tf.nn.leaky_relu (discriminator)\n",
    "        \n",
    "        discriminator = Conv2D(\n",
    "                        filters = self.filter1,\n",
    "                        kernel_size = self.kernel_size,\n",
    "                        strides = self.two_strides,\n",
    "                        padding = \"same\") (discriminator)\n",
    "        # discriminator = BatchNormalization() (discriminator)\n",
    "        discriminator = tf.nn.leaky_relu (discriminator)\n",
    "        \n",
    "        discriminator = Flatten()(discriminator)\n",
    "        discriminator = Dense(1, activation = None) (discriminator)\n",
    "        \n",
    "        outputs = discriminator\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def train_FUNCTION(self, epochs = 100, batch_size = 128):\n",
    "        \n",
    "        \n",
    "        # Create discriminator Model\n",
    "        discriminator_inputs = Input (shape = (28, 28, 1))\n",
    "        discriminator_outputs = self.build_Discriminator(discriminator_inputs)\n",
    "        DISCRIMINATOR = Model (inputs = discriminator_inputs, \n",
    "                               outputs = discriminator_outputs,\n",
    "                               name = \"DISCRIMIANTOR\")\n",
    "        DISCRIMINATOR.compile(optimizer = RMSprop(learning_rate = self.learning_rate, \n",
    "                                                  rho = 0.5),\n",
    "                              loss = 'mse',\n",
    "                              metrics = [\"accuracy\"])\n",
    "        DISCRIMINATOR.summary()\n",
    "\n",
    "        # Create generator model, But Do not compile\n",
    "        generator_inputs = Input (shape = (self.noise_dimenstion,))\n",
    "        generator_outputs = self.build_Generator(generator_inputs)\n",
    "        GENERATOR = Model(inputs = generator_inputs, \n",
    "                          outputs = generator_outputs, \n",
    "                          name = \"GENERATOR\")\n",
    "        GENERATOR.summary()\n",
    "        \n",
    "        # Last, combine discriminator + generator model.\n",
    "        # The model is not trained for all layers, only generator model\n",
    "        # So, We need discriminator of trainiable method. \n",
    "        entired_inputs = GENERATOR(generator_inputs)\n",
    "        entired_outputs = DISCRIMINATOR (entired_inputs)\n",
    "        LSGAN_MODEL = Model (inputs = generator_inputs, outputs = entired_outputs,\n",
    "             name = \"LSGAN_MODEL\")\n",
    "        LSGAN_MODEL.compile(optimizer = RMSprop(learning_rate = 0.5*self.learning_rate, \n",
    "                                                rho = 0.5),\n",
    "                            loss = 'mse', \n",
    "                            metrics = [\"accuracy\"])\n",
    "        LSGAN_MODEL.summary()\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        1. Load MNIST Data\n",
    "        2. \"real\" is target value of real image\n",
    "        3. \"fake\" is target value of fake image\n",
    "        \"\"\"\n",
    "        (train_image, _), (_, _) = mnist.load_data()\n",
    "        train_image = train_image / 255.\n",
    "        real, fake = np.ones((batch_size, 1)), np.zeros((batch_size, 1))\n",
    "        \n",
    "        # 1 epoch = 60,000 images\n",
    "        for epoch in range (epochs):\n",
    "            \n",
    "            for batch in range (int(60000/batch_size)):\n",
    "\n",
    "                # Select a random batch of images\n",
    "                select_index = np.random.randint(0, train_image.shape[0], batch_size)\n",
    "                select_image = train_image[select_index]\n",
    "                select_image = np.reshape(select_image, (-1, 28, 28, 1))\n",
    "\n",
    "                # random_noise를 발생시키고, Pick a generator_images\n",
    "                random_noise = np.random.normal(0., 1., size = ([batch_size, self.noise_dimenstion]))\n",
    "                generator_image = GENERATOR.predict(random_noise)\n",
    "\n",
    "                \"\"\"\n",
    "                Select a random batch of image\n",
    "                ---> np.random.randint(0, 60000, 128) : 0에서 60000 사이 중, 128개를 랜덤으로 고르는 것\n",
    "                \"\"\"\n",
    "                # Train the discriminator\n",
    "                # 진짜 이미지가 들어오면 1로 판단, 가짜 이미지가 들어오면 0으로 판단하게끔 훈련\n",
    "                discriminator_loss_real = DISCRIMINATOR.train_on_batch(select_image, real)\n",
    "                discriminator_loss_fake = DISCRIMINATOR.train_on_batch(generator_image, fake)\n",
    "                discriminator_loss = 0.5*(np.add(discriminator_loss_real, discriminator_loss_fake))\n",
    "\n",
    "                # Train the generator\n",
    "                # random_noise를 넣어서 가짜 이미지가 진짜 이미지처럼 보이기 위해 valid_target = 1이 되도록 학습\n",
    "                DISCRIMINATOR.trainable = False\n",
    "                generator_loss = LSGAN_MODEL.train_on_batch(random_noise, real)\n",
    "                generator_loss = 0.5 * np.float32(generator_loss)\n",
    "\n",
    "                print (\"%d %d [Disc loss: %f, acc.: %.2f%%] [Gene loss: %f, acc.: %.2f%%]\" \\\n",
    "                       %(epoch+1, batch+1, \n",
    "                         discriminator_loss[0], 100*discriminator_loss[1], \n",
    "                         generator_loss[0], 100*generator_loss[1]))\n",
    "\n",
    "\n",
    "                test_noise = np.random.normal (0., 1., (10, self.noise_dimenstion))\n",
    "                test_generator = GENERATOR.predict (test_noise)\n",
    "                test_generator = test_generator * 0.5 + 0.5\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Show fake images per epoch\n",
    "            \"\"\"\n",
    "            fig, ax = plt.subplots(1, 10, figsize = (20, 10))\n",
    "            for i in range(10):\n",
    "\n",
    "                ax[i].set_axis_off()\n",
    "                ax[i].imshow(np.reshape(test_generator[i], (28, 28)))\n",
    "\n",
    "            plt.show ()\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "r3cSGJ-FOKFw",
    "outputId": "35a60200-447f-4c7e-9b69-388df4b169df"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    LSGAN = LSGAN_model()\n",
    "    LSGAN.train_FUNCTION()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
